global:
  smtp_smarthost: '{{ env "SMTP_HOST" }}:{{ env "SMTP_PORT" }}'
  smtp_from: '{{ env "SMTP_FROM" }}'
  smtp_auth_username: '{{ env "SMTP_USERNAME" }}'
  smtp_auth_password: '{{ env "SMTP_PASSWORD" }}'
  slack_api_url: '{{ env "SLACK_WEBHOOK_URL" }}'

route:
  # Enhanced grouping for multi-source deduplication
  group_by: ['alertname', 'cluster', 'service', 'instance']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 24h
  receiver: 'default'
  routes:
    # Prometheus alerts - standard metrics-based alerts
    - match:
        source: prometheus
      routes:
        # Critical alerts from Prometheus
        - match:
            severity: critical
          receiver: 'critical-alerts'
          group_wait: 10s
          repeat_interval: 1h

        # License and authentication alerts from Prometheus
        - match_re:
            alertname: '(LicenseExpired|LicenseExpiring|ProxyLimitExceeded|DatabaseConnectionFailure)'
          receiver: 'license-alerts'
          group_wait: 15s
          repeat_interval: 6h

        # Performance and resource alerts from Prometheus
        - match_re:
            alertname: '(HighLatency|HighErrorRate|HighCPUUsage|HighMemoryUsage|HighConnectionCount)'
          receiver: 'performance-alerts'
          group_wait: 2m
          repeat_interval: 12h

        # Security alerts from Prometheus
        - match_re:
            alertname: '(HighAuthenticationFailureRate|PossibleDDoSAttack|TLSCertificateExpired|TLSCertificateExpiring)'
          receiver: 'security-alerts'
          group_wait: 30s
          repeat_interval: 2h

    # ELK Stack alerts - log-based anomalies and patterns
    - match:
        source: elasticsearch
      routes:
        # Critical log-based alerts
        - match:
            severity: critical
          receiver: 'critical-alerts'
          group_wait: 15s
          repeat_interval: 1h

        # Authentication failure patterns
        - match_re:
            alertname: '(AuthenticationFailureSpike|SuspiciousLoginPattern|BruteForceDetected)'
          receiver: 'security-alerts'
          group_wait: 45s
          repeat_interval: 2h

        # Application error patterns
        - match_re:
            alertname: '(ErrorRateSpike|UnhandledException|DatabaseErrorPattern)'
          receiver: 'performance-alerts'
          group_wait: 2m
          repeat_interval: 6h

    # Grafana alerts - dashboard-based anomaly detection
    - match:
        source: grafana
      routes:
        # Critical dashboard alerts
        - match:
            severity: critical
          receiver: 'critical-alerts'
          group_wait: 20s
          repeat_interval: 1h

        # Business metrics alerts
        - match_re:
            alertname: '(ServiceAvailabilityDrop|RequestVolumeAnomaly|PerformanceDegradation)'
          receiver: 'performance-alerts'
          group_wait: 3m
          repeat_interval: 8h

    # Fallback routes for alerts without source labels (backward compatibility)
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      repeat_interval: 1h

    # License and authentication alerts
    - match_re:
        alertname: '(LicenseExpired|LicenseExpiring|ProxyLimitExceeded|DatabaseConnectionFailure)'
      receiver: 'license-alerts'
      group_wait: 15s
      repeat_interval: 6h

    # Performance and resource alerts
    - match_re:
        alertname: '(HighLatency|HighErrorRate|HighCPUUsage|HighMemoryUsage|HighConnectionCount)'
      receiver: 'performance-alerts'
      group_wait: 2m
      repeat_interval: 12h

    # Security alerts
    - match_re:
        alertname: '(HighAuthenticationFailureRate|PossibleDDoSAttack|TLSCertificateExpired|TLSCertificateExpiring)'
      receiver: 'security-alerts'
      group_wait: 30s
      repeat_interval: 2h

receivers:
  # Default receiver
  - name: 'default'
    email_configs:
      - to: '{{ env "ALERT_EMAIL_DEFAULT" }}'
        subject: 'MarchProxy Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          Started: {{ .StartsAt }}
          {{ end }}

  # Critical alerts - multiple channels
  - name: 'critical-alerts'
    email_configs:
      - to: '{{ env "ALERT_EMAIL_CRITICAL" }}'
        subject: 'üö® CRITICAL: MarchProxy {{ .GroupLabels.alertname }}'
        body: |
          CRITICAL ALERT - IMMEDIATE ACTION REQUIRED

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          Started: {{ .StartsAt }}

          This alert requires immediate attention!
          {{ end }}
    # Uncomment and configure for Slack notifications
    # slack_configs:
    #   - api_url: '{{ .SlackAPIURL }}'
    #     channel: '#critical-alerts'
    #     title: 'CRITICAL: MarchProxy Alert'
    #     text: |
    #       {{ range .Alerts }}
    #       *{{ .Annotations.summary }}*
    #       {{ .Annotations.description }}
    #       Service: {{ .Labels.service }}
    #       {{ end }}
    # webhook_configs:
    #   - url: 'https://your-pagerduty-webhook-url'
    #     send_resolved: true

  # License and business continuity alerts
  - name: 'license-alerts'
    email_configs:
      - to: '{{ env "ALERT_EMAIL_LICENSE" }}'
        subject: '‚ö†Ô∏è MarchProxy License Alert: {{ .GroupLabels.alertname }}'
        body: |
          LICENSE/BUSINESS CONTINUITY ALERT

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Started: {{ .StartsAt }}

          Action Required: Please check license status and renewal requirements.
          {{ end }}

  # Performance alerts
  - name: 'performance-alerts'
    email_configs:
      - to: '{{ env "ALERT_EMAIL_PERFORMANCE" }}'
        subject: 'üìä MarchProxy Performance Alert: {{ .GroupLabels.alertname }}'
        body: |
          PERFORMANCE ALERT

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          Started: {{ .StartsAt }}
          {{ end }}

  # Security alerts
  - name: 'security-alerts'
    email_configs:
      - to: '{{ env "ALERT_EMAIL_SECURITY" }}'
        subject: 'üîí MarchProxy Security Alert: {{ .GroupLabels.alertname }}'
        body: |
          SECURITY ALERT

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          Started: {{ .StartsAt }}

          This may indicate a security incident requiring investigation.
          {{ end }}

inhibit_rules:
  # Cross-source deduplication: Inhibit non-critical alerts when critical alerts are firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['service', 'instance']

  # Multi-source service outage deduplication
  - source_match:
      alertname: 'MarchProxyManagerDown'
    target_match_re:
      alertname: '(DatabaseConnectionFailure|LicenseExpired|ManagerHealthCheckFailed)'
    equal: ['instance']

  # Cross-source authentication alert deduplication
  - source_match:
      alertname: 'HighAuthenticationFailureRate'
      source: 'prometheus'
    target_match_re:
      alertname: '(AuthenticationFailureSpike|SuspiciousLoginPattern)'
      source: 'elasticsearch'
    equal: ['service', 'instance']

  # Cross-source performance alert deduplication
  - source_match:
      alertname: 'HighLatency'
      source: 'prometheus'
    target_match_re:
      alertname: '(PerformanceDegradation|RequestVolumeAnomaly)'
      source: 'grafana'
    equal: ['service', 'instance']

  # ELK and Grafana error correlation deduplication
  - source_match:
      alertname: 'ErrorRateSpike'
      source: 'elasticsearch'
    target_match:
      alertname: 'ServiceAvailabilityDrop'
      source: 'grafana'
    equal: ['service']

  # Prometheus metric alerts take precedence over derived dashboard alerts
  - source_match_re:
      alertname: '(HighCPUUsage|HighMemoryUsage|HighConnectionCount)'
      source: 'prometheus'
    target_match:
      alertname: 'PerformanceDegradation'
      source: 'grafana'
    equal: ['service', 'instance']

  # Infrastructure alerts suppress application-level symptoms
  - source_match:
      alertname: 'DatabaseConnectionFailure'
    target_match_re:
      alertname: '(DatabaseErrorPattern|UnhandledException)'
    equal: ['service']

  # Network-level alerts suppress application symptoms
  - source_match_re:
      alertname: '(PossibleDDoSAttack|NetworkConnectivityIssue)'
    target_match_re:
      alertname: '(RequestVolumeAnomaly|ServiceAvailabilityDrop)'
    equal: ['instance']